\documentclass[12pt]{article}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{graphicx, lipsum,caption}
\usepackage[colorinlistoftodos]{todonotes}

\begin{document}
	
	\begin{titlepage}
		
		\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
		
		\center % Center everything on the page
		
		%----------------------------------------------------------------------------------------
		%	HEADING SECTIONS
		%----------------------------------------------------------------------------------------
		
		\textsc{\LARGE Portland State University}\\[1.5cm] % Name of your university/college
		\textsc{\Large Deep Learning: Computational Structures and Programming}\\[0.5cm] % Major heading such as course name
		\textsc{\large Winter 2021}\\[0.5cm] % Minor heading such as course title
		
		%----------------------------------------------------------------------------------------
		%	TITLE SECTION
		%----------------------------------------------------------------------------------------
		
		\HRule \\[0.4cm]
		{ \huge \bfseries Project 1}\\[0.4cm] % Title of your document
		\HRule \\[1.5cm]
		
		%----------------------------------------------------------------------------------------
		%	AUTHOR SECTION
		%----------------------------------------------------------------------------------------
		
		\begin{minipage}{0.4\textwidth}
			\begin{flushleft} \large
				\emph{Author:}\\
				Hermann \textsc{Yepdjio} % Your name
			\end{flushleft}
		\end{minipage}
		~
		\begin{minipage}{0.4\textwidth}
			\begin{flushright} \large
				\emph{Professor:} \\
				Dr. Suresh \textsc{Singh} % Supervisor's Name
			\end{flushright}
		\end{minipage}\\[1cm]
		
		% If you don't want a supervisor, uncomment the two lines below and remove the section above
		%\Large \emph{Author:}\\
		%John \textsc{Smith}\\[3cm] % Your name
		
		%----------------------------------------------------------------------------------------
		%	DATE SECTION
		%----------------------------------------------------------------------------------------
		
		{\large \today}\\[0.7cm] % Date, change the \today to a set date if you want to be precise
		
		%----------------------------------------------------------------------------------------
		%	LOGO SECTION
		%----------------------------------------------------------------------------------------
		
		\includegraphics[width=7cm]{PSU_LOGO.png}\\[.5cm] % Include a department/university logo - this will require the graphicx package
		
		%----------------------------------------------------------------------------------------
		
		\vfill % Fill the rest of the page with whitespace
		
	\end{titlepage}
	%\newpage
	%\tableofcontents
	%\newpage
	
	
	
	\section*{Submission Report}
	
	
		1) Link to my colab notebook:\\
		 \url{https://colab.research.google.com/drive/1Bx4EqzWYnoHE4LQCUDEdUDcqPrIBqTMK?usp=sharing}
		
		2) We implemented two different networks with the following configurations: \\
		\begin{itemize}
			\item \bf{Network 1:} one input layer, one hidden layer, and one output layer of 784, 1024, and 10 neurons respectively. The activation and loss functions used were ReLU and cross entropy respectively. We chose optim.SGD as the optimizer with momentum 0 and learning rate 0.001. We used a mini-batch size of 30.
			\item \bf{Network 2:} similar to Network 1 except that we added another hidden layer of 1024 neurons.
		\end{itemize}
		
		After training (for 10 epochs) and testing both models on the FashionMNIST dataset, we obtained accuracies of $80\%$ and $79\%$ respectively for Network 1 and Network 2. The fact that the latter performed slightly worse than the former despite having more neurons is probably due to the first having more neurons than it was needed. This probably has caused it to start memorizing rather than generalizing the training set. \\
		Another explanation could be that, Network 2 needed to train for more epochs given its higher number of neurons. In fact, when looking at the loss after each epoch, both models started at $loss \approx 1.1$ at epoch 1, but Network 1 reached $loss \approx 0.09$ at epoch 10 while Network 2 was still at $loss \approx 0.4$. 
		
		3) Here, we used Network 2 mentioned above and adjusted some its parameters as indicated in Table 1. The results that we obtained as also recorded in the same table.\\
		As we can see, the best accuracy ($88\%$) was obtained when using ReLU activation function, $mini-batch-size = 1$ and $learning-rate = 0.01$ while the worst accuracy ($10$) mostly was obtained when using $learning-rate = 1.0$. The difference can be explained by the fact that a learning rate of $1.0$ is probably too high and prevent the network to learn effectively during training because it is trying to learning too fast. 
		\newpage
		\begin{table}[h]
			\centering
			\caption{}
			\label{tab:my-table}
			\begin{tabular}{lccclll}
				\hline
				\begin{tabular}[c]{@{}l@{}}Mini\_Batch Size /\\ Learning Rate\end{tabular} &
				\multicolumn{1}{l}{1 (ReLU)} &
				\multicolumn{1}{l}{10 (ReLU)} &
				\multicolumn{1}{l}{1000 (ReLU)} &
				1 (Sig) &
				10 (Sig) &
				1000 (Sig) \\ \hline
				1.0   & 10\% & 10\% & 10\% & 10\% & 86\% & 67\% \\
				0.1   & 10\% & 87\% & 81\% & 87\% & 86\% & 61\% \\
				0.01  & 88\% & 87\% & 67\% & 85\% & 82\% & 10\% \\
				0.001 & 87\% & 83\% & 43\% & 81\% & 62\% & 19\% \\ \hline
			\end{tabular}
		\end{table}
		
		
		4) In this part of the assignment we polluted the training set by Picking 9 sets of $1\%$ of images from each of the 10 categories and adding them to the other 9 categories. We used the best model mentioned in the previous paragraph and we obtained exactly the same result ($88\%$ classification accuracy). However, if we look at the loss after each epoch, when the data is not polluted, the loss varies between 0.805 (epoch 1) and 0.203 (epoch 10). On the other hand when the data is polluted the loss varies between 1.045 (epoch 1) and 0.511 (epoch 10). Therefore, in the second scenario it is possible that the model realized that a small portion of data in each category did not belong that category and ignored that portion when updating the weights and bias.  
	%\bibliographystyle{ieeetr}
	%\bibliography{references}
	
\end{document}
